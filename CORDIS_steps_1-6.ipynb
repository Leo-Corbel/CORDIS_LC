{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3e2d6c30-efd6-4cff-bb83-84ed2a33ceac",
   "metadata": {},
   "source": [
    "=======================================PIPELINE_CLEAN_VERSION_24_06_2025================================================\n",
    "STEP 1: MERGER LES DIFFERENTS FICHIERS DES 9 FRAMEWORK PROG (+suppr lignes vides; +Namekey)\n",
    "        R1_FP.csv\n",
    "STEP 2: TROUVER LES SERVICES GEOLOGIQUES PAR LA RACINE \"GEO\" (dans NAMEKEY)\n",
    "        R2_geo_unique.csv\n",
    "STEP 3: CONSITUER UN REPERTOIRE DES OCCURENCES UNIQUES. TRI MANUEL DES SERVICES GEOLOGIQUES (\"GEOL\" & \"GEO\")\n",
    "        R3_repertoire_GS.csv\n",
    "STEP 4: CONSTRUCTION MANUELLE D'UN REFERENTIEL D'HARMONISATION DES SERVICES GEOLOGIQUES (28 services géologiques de l'UE)\n",
    "        R4_GS_root.csv\n",
    "STEP 5: DETECTER LES SERVICES GEOLOGIQUES PAR ITERATION DANS L'ENSEMBLE DE LA BASE DE DONNEES (1352,)\n",
    "        R5_all_GS_detected.csv\n",
    "STEP 6: TROUVER LES PARTENAIRES DE PROJETS DES SERVICES GEOLOGIQUES DANS LA BASE FP (13918,)\n",
    "        R6_GS_and_partners.csv\n",
    "STEP 7: STANDARDISER LES DONNEES DES SERVICES GEOLOGIQUES A PARTIR D'UN REFERENTIEL COMMUN (28 UE GS pour 28 PAYS) (1352,68)\n",
    "        R7_GS_harmonized_and_partners.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf77c623-766f-488d-8fea-6f6cdd6b5679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporté : C:\\Users\\Leo\\Desktop\\CORDIS\\RESULTS\\Enriched_FP1.csv\n",
      "Exporté : C:\\Users\\Leo\\Desktop\\CORDIS\\RESULTS\\Enriched_FP2.csv\n",
      "Exporté : C:\\Users\\Leo\\Desktop\\CORDIS\\RESULTS\\Enriched_FP3.csv\n",
      "Exporté : C:\\Users\\Leo\\Desktop\\CORDIS\\RESULTS\\Enriched_FP4.csv\n",
      "Exporté : C:\\Users\\Leo\\Desktop\\CORDIS\\RESULTS\\Enriched_FP5.csv\n",
      "Exporté : C:\\Users\\Leo\\Desktop\\CORDIS\\RESULTS\\Enriched_FP6.csv\n",
      "Exporté : C:\\Users\\Leo\\Desktop\\CORDIS\\RESULTS\\Enriched_FP7.csv\n",
      "Exporté : C:\\Users\\Leo\\Desktop\\CORDIS\\RESULTS\\Enriched_H2020.csv\n",
      "Exporté : C:\\Users\\Leo\\Desktop\\CORDIS\\RESULTS\\Enriched_HORIZON.csv\n",
      "Avant filtrage : (699912, 49)\n",
      "Après filtrage : (699212, 49)\n",
      "Export global nettoyé : C:\\Users\\Leo\\Desktop\\CORDIS\\RESULTS\\R1_FP.csv\n"
     ]
    }
   ],
   "source": [
    "# ETAPE 1 - MERGER LES DIFFERENTS FICHIERS DES 9 FRAMEWORK PROGRAMME\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from unidecode import unidecode\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "raw_dir = Path(r\"C:\\Users\\Leo\\Desktop\\CORDIS\\RAW\")\n",
    "result_dir = Path(r\"C:\\Users\\Leo\\Desktop\\CORDIS\\RESULTS\")\n",
    "fp_versions = ['FP1', 'FP2', 'FP3', 'FP4', 'FP5', 'FP6', 'FP7', 'H2020', 'HORIZON']\n",
    "\n",
    "org_paths = [raw_dir / version / \"organization.csv\" for version in fp_versions]\n",
    "proj_paths = [raw_dir / version / \"project.csv\" for version in fp_versions]\n",
    "topics_paths = [raw_dir / version / \"topics.csv\" for version in fp_versions]\n",
    "euroscivoc_paths = [raw_dir / version / \"euroSciVoc.csv\" for version in fp_versions]\n",
    "legal_paths = [raw_dir / version / \"legalBasis.csv\" for version in fp_versions]\n",
    "\n",
    "# === FONCTIONS ===\n",
    "def load_dataset(path):\n",
    "    if path.exists():\n",
    "        try:\n",
    "            return pd.read_csv(path, sep=None, engine='python', on_bad_lines='skip')\n",
    "        except UnicodeDecodeError:\n",
    "            return pd.read_csv(path, sep=None, engine='python', encoding='ISO-8859-1', on_bad_lines='skip')\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def clean_dataframe(df, key_column):\n",
    "    df = df.copy()\n",
    "    if key_column in df.columns:\n",
    "        df = df[df[key_column].notna()]\n",
    "        df[key_column] = df[key_column].astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "def aggregate_by_project(df, key, value_cols):\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[key] + value_cols)\n",
    "    df[key] = df[key].astype(str).str.strip()\n",
    "    return df.groupby(key)[value_cols].agg(lambda x: '; '.join(sorted(set(x.dropna().astype(str))))).reset_index()\n",
    "\n",
    "def process_fp(fp_idx, version):\n",
    "    org = clean_dataframe(load_dataset(org_paths[fp_idx]), \"projectID\")\n",
    "    proj = clean_dataframe(load_dataset(proj_paths[fp_idx]), \"id\")\n",
    "    topics = aggregate_by_project(load_dataset(topics_paths[fp_idx]), \"projectID\", [\"title\"])\n",
    "    scivoc = aggregate_by_project(load_dataset(euroscivoc_paths[fp_idx]), \"projectID\", [\"euroSciVocTitle\"])\n",
    "    legal = aggregate_by_project(load_dataset(legal_paths[fp_idx]), \"projectID\", [\"legalBasis\"])\n",
    "\n",
    "    enriched = org\n",
    "    if not proj.empty:\n",
    "        enriched = enriched.merge(proj, left_on=\"projectID\", right_on=\"id\", how=\"left\")\n",
    "    if not topics.empty:\n",
    "        enriched = enriched.merge(topics, on=\"projectID\", how=\"left\")\n",
    "    if not scivoc.empty:\n",
    "        enriched = enriched.merge(scivoc, on=\"projectID\", how=\"left\")\n",
    "    if not legal.empty:\n",
    "        enriched = enriched.merge(legal, on=\"projectID\", how=\"left\")\n",
    "\n",
    "    enriched['frameworkProgramme'] = version.upper()\n",
    "\n",
    "    out_path = result_dir / f\"Enriched_{version}.csv\"\n",
    "    enriched.to_csv(out_path, index=False)\n",
    "    print(f\"Exporté : {out_path}\")\n",
    "    return enriched\n",
    "\n",
    "def clean_and_normalize(text):\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    return unidecode(text).lower().strip()\n",
    "\n",
    "# === TRAITEMENT GLOBAL ===\n",
    "all_data = []\n",
    "\n",
    "for i, version in enumerate(fp_versions):\n",
    "    enriched_df = process_fp(i, version)\n",
    "    all_data.append(enriched_df)\n",
    "\n",
    "if all_data:\n",
    "    full_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    # === FILTRAGE : supprimer les lignes sans 'name' ni 'shortName'\n",
    "    print(\"Avant filtrage :\", full_df.shape)\n",
    "    full_df = full_df[~(\n",
    "        (full_df['name'].fillna('').str.strip() == '') &\n",
    "        (full_df['shortName'].fillna('').str.strip() == '')\n",
    "    )]\n",
    "    print(\"Après filtrage :\", full_df.shape)\n",
    "\n",
    "    # === AJOUT DE LA COLONNE NORMALISÉE 'namekey'\n",
    "    full_df['namekey'] = full_df['name'].fillna('').apply(clean_and_normalize)\n",
    "\n",
    "    # === EXPORT DES DONNÉES NETTOYÉES ===\n",
    "    final_path = result_dir / \"R1_FP.csv\"\n",
    "    full_df.to_csv(final_path, index=False)\n",
    "    print(f\"Export global nettoyé : {final_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cef71032-03eb-40f7-abd8-78d12cb86af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Export effectué : C:\\Users\\Leo\\Desktop\\CORDIS\\RESULTS\\R2_geo_unique.csv\n",
      "(1257, 50)\n"
     ]
    }
   ],
   "source": [
    "# ETAPE 2 - IDENTIFIER LES SERVICES GEOLOGIQUES PAR LES RACINES \"geo\" / \"geol\" DANS \"namekey\"\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "result_dir = Path(r\"C:\\Users\\Leo\\Desktop\\CORDIS\\RESULTS\")\n",
    "input_file = result_dir / \"R1_FP.csv\"\n",
    "output_file = result_dir / \"R2_geo_unique.csv\"\n",
    "\n",
    "# === CHARGEMENT DES DONNÉES ===\n",
    "df = pd.read_csv(input_file, low_memory=False)\n",
    "\n",
    "# === FILTRAGE : ORGANISATIONS CONTENANT \"geo\" OU \"geol\" DANS namekey ===\n",
    "filtered_df = df[df['namekey'].str.contains(r'geo|geol', case=False, na=False)]\n",
    "\n",
    "# === EXTRACTION DES ENTRÉES UNIQUES PAR namekey ===\n",
    "unique_geol_df = df[df['namekey'].isin(filtered_df['namekey'])].drop_duplicates(subset='namekey')\n",
    "\n",
    "# === EXPORT DU RÉSULTAT FILTRÉ ===\n",
    "unique_geol_df.to_csv(output_file, index=False)\n",
    "print(f\"\\nExport effectué : {output_file}\")\n",
    "print(unique_geol_df.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05489699-0235-4c82-bb33-62af4e5b7361",
   "metadata": {},
   "source": [
    "#ETAPE 3 - CONSITUER UN REPERTOIRE DES OCCURENCES UNIQUES / TRIER LES SERVICES GEOLOGIQUES\n",
    "\n",
    "** I/ DETECTER LES SERVICES GEOLOGIQUES DANS LA BASE DE DONNEES PAR LA RACINE \"GEOL\" **\n",
    "\n",
    "QU'EST-CE QU UN SERVICE GEOLOGIQUE ?\n",
    "=> On ne garde pas EUROGEOSURVEYS § \n",
    "=> On ne garde pas les services géologiques régionaux (landers; irlande du Nord)\n",
    "=> On garde seulement les 27 + 1 (UK)\n",
    "=> On ne garde pas le reste; associations, sociétés privés, départements de géologie universitaires, etc...\n",
    "=> Pour les acronymes ; on garde l'ensemble tout en évitant certains (sans TNO on passe de 167 occ à 33)\n",
    "=> Ceux qui ont été oubliés seront rattrapés par \"capilarité\" des participants aux projets \n",
    "\n",
    "** II/ ON AJOUTE LA DETECTION DES SHORTNAME / ACRONYMES DES SERVICES GEOLOGIQUES (BRGM, BGS, etc.)**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a61e5124-f0b3-435d-98b7-9013ab0d9535",
   "metadata": {},
   "source": [
    "#ETAPE 4 - CONSTRUIRE UN référentiel d’uniformisation des noms pour les services géologiques (28 services géologiques de l'UE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79e8661f-2cba-47d4-8894-d465e97a8ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export terminé : C:\\Users\\Leo\\Desktop\\CORDIS\\RESULTS\\R5_all_GS_detected.csv\n",
      "\n",
      "Méthodes de détection des GS :\n",
      "GS_detected_by\n",
      "                     697860\n",
      "namekey                1319\n",
      "shortname+country        33\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ETAPE 5 : DÉTECTER LES SERVICES GÉOLOGIQUES DANS LA BASE POUR LES UNIFORMISER\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from unidecode import unidecode\n",
    "\n",
    "# === Chargement des fichiers ===\n",
    "base = Path(r\"C:\\Users\\Leo\\Desktop\\CORDIS\\RESULTS\")\n",
    "df_main = pd.read_csv(base / \"R1_FP.csv\", encoding='utf-8-sig', low_memory=False)\n",
    "df_geol = pd.read_csv(base / \"R3_repertoire_GS.csv\", encoding='utf-8-sig', delimiter=';', low_memory=False)\n",
    "\n",
    "# === Normalisation des champs utiles ===\n",
    "def normalize(s):\n",
    "    return unidecode(str(s).strip().upper()) if pd.notna(s) else None\n",
    "\n",
    "# namekey et shortname doivent être normalisés (résultat du test précédent)\n",
    "df_main['namekey_norm'] = df_main['namekey'].apply(normalize)\n",
    "df_main['shortname_norm'] = df_main['shortName'].apply(normalize)\n",
    "df_main['country_norm'] = df_main['country']  # pas besoin de normaliser, déjà propre\n",
    "\n",
    "df_geol['namekey_norm'] = df_geol['namekey'].apply(normalize)\n",
    "df_geol['shortname_norm'] = df_geol['shortname'].apply(normalize)\n",
    "df_geol['country_norm'] = df_geol['country'].apply(normalize)\n",
    "\n",
    "# === Étape 1 : Détection par namekey exact ===\n",
    "set_namekeys = set(df_geol['namekey_norm'].dropna())\n",
    "df_main['GS'] = df_main['namekey_norm'].isin(set_namekeys).astype(int)\n",
    "df_main['GS_detected_by'] = df_main['GS'].apply(lambda x: 'namekey' if x == 1 else '')\n",
    "\n",
    "# === Étape 2 : Détection par (shortname + country) si GS == 0 ===\n",
    "geo_acronyms = set(\n",
    "    zip(\n",
    "        df_geol.loc[df_geol['shortname_norm'].notna(), 'shortname_norm'],\n",
    "        df_geol.loc[df_geol['shortname_norm'].notna(), 'country_norm']\n",
    "    )\n",
    ")\n",
    "\n",
    "mask = df_main['GS'] == 0\n",
    "matches = df_main[mask].apply(\n",
    "    lambda row: (row['shortname_norm'], row['country_norm']) in geo_acronyms,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_main.loc[mask & matches, 'GS'] = 1\n",
    "df_main.loc[mask & matches, 'GS_detected_by'] = 'shortname+country'\n",
    "\n",
    "# === Export final ===\n",
    "output_path = base / \"R5_all_GS_detected.csv\"\n",
    "df_main.to_csv(output_path, index=False, sep=';', encoding='utf-8-sig')\n",
    "print(f\"Export terminé : {output_path}\")\n",
    "\n",
    "# === Résumé des méthodes de détection ===\n",
    "print(\"\\nMéthodes de détection des GS :\")\n",
    "print(df_main['GS_detected_by'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc88f9e0-cc6f-4fab-8893-0f51d254d755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total : 13918\n"
     ]
    }
   ],
   "source": [
    "#STEP 6: TROUVER LES PARTENAIRES DE PROJETS DES SERVICES GEOLOGIQUES DANS LA BASE FP\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Chemin vers les résultats\n",
    "result_dir = Path(r\"C:\\Users\\Leo\\Desktop\\CORDIS\\RESULTS\")\n",
    "\n",
    "# Charger les fichiers nécessaires\n",
    "file_organisations = result_dir / \"R1_FP.csv\"\n",
    "file_geological_surveys = result_dir / \"R5_all_GS_detected.csv\"\n",
    "\n",
    "# Lire les données\n",
    "df_orgs = pd.read_csv(file_organisations, encoding='utf-8-sig', low_memory=False)\n",
    "df_geo_orgs = pd.read_csv(file_geological_surveys, encoding='utf-8-sig', delimiter=';', low_memory=False)\n",
    "\n",
    "# Ne garder que les lignes identifiées comme GS\n",
    "df_geo_orgs = df_geo_orgs[df_geo_orgs['GS'] == 1]\n",
    "\n",
    "# Définir les groupes de programmes\n",
    "fp_groups = {\n",
    "    'FP1_FP6': ('rcn_y', ['FP1', 'FP2', 'FP3', 'FP4', 'FP5', 'FP6']),\n",
    "    'FP7_H2020': ('id', ['FP7', 'H2020', 'HORIZON'])\n",
    "}\n",
    "\n",
    "# Extraire les partenaires\n",
    "df_partners = []\n",
    "\n",
    "for label, (key, fps) in fp_groups.items():\n",
    "    df_geo_subset = df_geo_orgs[df_geo_orgs['frameworkProgramme'].isin(fps)]\n",
    "    geo_ids = df_geo_subset[key].dropna().unique()\n",
    "    df_fp_partners = df_orgs[\n",
    "        (df_orgs['frameworkProgramme'].isin(fps)) &\n",
    "        (df_orgs[key].isin(geo_ids))\n",
    "    ]\n",
    "    df_partners.append(df_fp_partners)\n",
    "\n",
    "# Concaténer tous les résultats\n",
    "df_all_geo_partners = pd.concat(df_partners + [df_geo_orgs], ignore_index=True)\n",
    "\n",
    "# Sauvegarder le fichier final\n",
    "output_path = result_dir / \"R6_GS_and_partners.csv\"\n",
    "df_all_geo_partners.to_csv(output_path, index=False, sep=';', encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Total : {df_all_geo_partners.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fb70ce1-9014-47ae-aac4-209dce2066c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lignes harmonisées : 1352\n",
      "Colonnes ajoutées : ['GS_name', 'GS_shortname', 'GS_street', 'GS_postcode', 'GS_city', 'GS_latitude', 'GS_longitude']\n"
     ]
    }
   ],
   "source": [
    "#STEP 7: STANDARDISER LES SERVICES GEOLOGIQUES A PARTIR DU REFERENIEL (STEP 4. \"R4_GS_root.csv\")\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from unidecode import unidecode\n",
    "\n",
    "# === Fonction de normalisation ===\n",
    "def normalize(s):\n",
    "    return unidecode(str(s).strip().upper()) if pd.notna(s) else None\n",
    "\n",
    "# === Chargement des fichiers ===\n",
    "base = Path(r\"C:\\Users\\Leo\\Desktop\\CORDIS\\RESULTS\")\n",
    "df_main = pd.read_csv(base / \"R6_GS_and_partners.csv\", encoding='utf-8-sig', sep=';', low_memory=False)\n",
    "df_ref = pd.read_csv(base / \"R4_GS_root.csv\", encoding='utf-8-sig', sep=';')\n",
    "\n",
    "# === Normalisation des pays pour jointure ===\n",
    "df_main['country_norm'] = df_main['country'].apply(normalize)\n",
    "df_ref['country_norm'] = df_ref['country'].apply(normalize)\n",
    "\n",
    "# === Jointure avec le référentiel (ajout des données officielles) ===\n",
    "df_merged = df_main.merge(\n",
    "    df_ref.add_prefix(\"GS_\"),\n",
    "    how='left',\n",
    "    left_on='country_norm',\n",
    "    right_on='GS_country_norm'\n",
    ")\n",
    "\n",
    "# === Liste des champs à harmoniser depuis le référentiel ===\n",
    "fields_to_harmonize = ['name', 'shortname', 'street', 'postcode', 'city', 'latitude', 'longitude']\n",
    "\n",
    "# === Harmonisation conditionnelle (uniquement si GS détecté) ===\n",
    "for field in fields_to_harmonize:\n",
    "    df_merged.loc[df_merged['GS'] == 1, field] = df_merged.loc[df_merged['GS'] == 1, f'GS_{field}']\n",
    "\n",
    "# === Export du résultat harmonisé ===\n",
    "output_path = base / \"R7_GS_harmonized_and_partners.csv\"\n",
    "df_merged.to_csv(output_path, index=False, sep=';', encoding='utf-8-sig')\n",
    "\n",
    "# === Résumé du traitement ===\n",
    "print(\"Lignes harmonisées :\", (df_merged['GS'] == 1).sum())\n",
    "print(\"Colonnes ajoutées :\", [f'GS_{col}' for col in fields_to_harmonize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aaf9fe-7437-49b0-a5c7-1aa383adc96f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
